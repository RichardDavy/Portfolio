{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b73634d",
   "metadata": {},
   "source": [
    "# <center>Creating a Spam Filter with a Naive Bayes Alogrithm</center>\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"Data/GettyImages-122143117-5c64996246e0fb0001f256b1.jpg\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "In this project, we're going to study the practical side of the naive Bayes algorithm by building a spam filter for SMS messages.\n",
    "\n",
    "To classify messages as spam or non-spam, we can utilise a naive Bayes algorith that:\n",
    "\n",
    "1. Learns how humans classify messages.\n",
    "2. Uses that human knowledge to estimate probabilities for new messages — probabilities for spam and non-spam.\n",
    "3. Classifies a new message based on these probability values — if the probability for spam is greater, then it classifies the message as spam. Otherwise, it classifies it as non-spam (if the two probability values are equal, then we may need a human to classify the message).\n",
    "\n",
    "So our first task is to \"teach\" the computer how to classify messages. To do that, we'll use the multinomial Naive Bayes algorithm along with a dataset of 5,572 SMS messages that are already classified by humans.\n",
    "\n",
    "The dataset was put together by Tiago A. Almeida and José María Gómez Hidalgo, and it can be downloaded from the [The UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). You can also download the dataset directly from this [link](https://dq-content.s3.amazonaws.com/433/SMSSpamCollection). The data collection process is described in more details on [this page](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/#composition), where you can also find some of the authors' papers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b94b96",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad3a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import regex as re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850f23a7",
   "metadata": {},
   "source": [
    "# Reading in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9220543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the spam message data.\n",
    "# The data is a tab delimited file with no headers - so we are accounting for this in the read_csv function.\n",
    "spam_data = pd.read_csv('./Data/SMSSpamCollection', sep='\\t', header=None, names=['Label', 'SMS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c89baa",
   "metadata": {},
   "source": [
    "# Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e98eaf4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First five entries:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Last five entries:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                                SMS\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'There are 5572 rows in the dataset'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'There are 2 columns in the dataset'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exploring the dataset \n",
    "display('First five entries:')\n",
    "display(spam_data.head(5))\n",
    "display('Last five entries:')\n",
    "display(spam_data.tail(5))\n",
    "\n",
    "# Figuring out the dimensions of the dataset\n",
    "display('There are ' + str(spam_data.shape[0]) + ' rows in the dataset')\n",
    "display('There are ' + str(spam_data.shape[1]) + ' columns in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee2babe",
   "metadata": {},
   "source": [
    "Spam messages are appropriately labelled <mark>spam</mark>, while non-spam messages are labelled <mark>ham</mark>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab50f760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     86.593683\n",
       "spam    13.406317\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the proportion of spam to non-spam messages.\n",
    "\n",
    "spam_data['Label'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d7a1ac",
   "metadata": {},
   "source": [
    "We can see that the 86.6% of the messages are genuine, while the remaining 13.4% are spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8a88c0",
   "metadata": {},
   "source": [
    "## Creating training and test datasets.\n",
    "\n",
    "Now that we've become a bit familiar with the dataset, we can move on to building the spam filter.\n",
    "\n",
    "However, before creating it, it's very helpful to first think of a way of testing how well it works. If we write the software first, then it's tempting to come up with a biased test just to make sure the software passes it.\n",
    "\n",
    "To test the spam filter, we're first going to split our dataset into two categories:\n",
    "\n",
    "- A __training__ set, which we'll use to \"train\" the computer how to classify messages.\n",
    "- A __test__ set, which we'll use to test how good the spam filter is with classifying new messages.\n",
    "\n",
    "We're going to keep 80% of our dataset for training, and 20% for testing (we want to train the algorithm on as much data as possible, but we also want to have enough test data). The dataset has 5,572 messages, which means that:\n",
    "\n",
    "- The __training__ set will have 4,458 messages (about 80% of the dataset).\n",
    "- The __test__ set will have 1,114 messages (about 20% of the dataset).\n",
    "\n",
    "We're going to randomise the entire dataset to ensure that spam and ham messages are spread properly throughout the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68500bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 4458 rows in the training dataset'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'There are 1114 rows in the test dataset'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Randomising the order of the dataset, incase there are any inherent ordering of the messages.\n",
    "spam_data_randomised = spam_data.sample(frac=1, random_state=1)\n",
    "\n",
    "# Setting the length of the training dataset for splitting the randomised dataset.\n",
    "length_training_ds = int(round(len(spam_data_randomised) * .8, 0))\n",
    "\n",
    "# Resetting the index as we have randomised all the entries.\n",
    "spam_data_randomised = spam_data_randomised.reset_index(drop=True)\n",
    "\n",
    "# Splitting based on index:\n",
    "training_ds = spam_data_randomised[:length_training_ds]\n",
    "test_ds = spam_data_randomised[length_training_ds:].reset_index(drop=True)\n",
    "\n",
    "# Figuring out the dimensions of the dataset\n",
    "display('There are ' + str(training_ds.shape[0]) + ' rows in the training dataset')\n",
    "\n",
    "display('There are ' + str(test_ds.shape[0]) + ' rows in the test dataset')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b0534a",
   "metadata": {},
   "source": [
    "We can see that we have successfully randomised and divided the dataset into a training and test set. Next we want to check that these datasets have proportions of spam and non-spam messages that are consistent with the original dataset we received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ea5a5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original dataset has the following proportions of spam and non-spam:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ham     86.593683\n",
       "spam    13.406317\n",
       "Name: Label, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sampled test dataset has the following proportions of spam and non-spam:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ham     86.804309\n",
       "spam    13.195691\n",
       "Name: Label, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sampled training dataset has the following proportions of spam and non-spam:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ham     86.54105\n",
       "spam    13.45895\n",
       "Name: Label, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculating the proportion of spam and non-spam messages using pd.value_counts\n",
    "\n",
    "print('The original dataset has the following proportions of spam and non-spam:')\n",
    "display(spam_data['Label'].value_counts(normalize=True)*100)\n",
    "\n",
    "print('The sampled test dataset has the following proportions of spam and non-spam:')\n",
    "display(test_ds['Label'].value_counts(normalize=True)*100)\n",
    "\n",
    "print('The sampled training dataset has the following proportions of spam and non-spam:')\n",
    "display(training_ds['Label'].value_counts(normalize=True)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425916d9",
   "metadata": {},
   "source": [
    "## Training Our Algorithm to Identify Spam SMS\n",
    "\n",
    "We have split our dataset into a training set and a test set. The next big step is to use the training set to teach the algorithm to classify new messages.\n",
    "\n",
    "\n",
    "Our Naive Bayes algorithm will make the classification based on the results it gets to these two equations:\n",
    "$$P(Spam|w_1,w_2,...,w_n)\\propto P(Spam)\\cdot\\prod_{i=1}^n P(w_i|Spam)$$\n",
    "\n",
    "$$P(Ham|w_1,w_2,...,w_n)\\propto P(Ham)\\cdot\\prod_{i=1}^n P(w_i|Ham)$$\n",
    "\n",
    "Also, to calculate P(wi|Spam) and P(wi|Ham) inside the formulas above, we need to use these equations:\n",
    "\n",
    "$$P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha\\cdot N_{Vocabulary}}$$\n",
    "\n",
    "$$P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha\\cdot N_{Vocabulary}}$$\n",
    "\n",
    "Summarising the terms in the equations above:\n",
    "\n",
    "$N_{w_i|Spam}$ = the number of times the word $w_i$ occurs in spam messages.\n",
    "\n",
    "$N_{w_i|Ham}$ = the number of times the word $w_i$ occurs in non-spam messages.\n",
    "\n",
    "$N_{Spam}$ = total number of words in spam messages.\n",
    "\n",
    "$N_{Ham}$ = total number of words in non-spam messages.\n",
    "\n",
    "$N_{Vocabulary}$ = total number of words in the vocabulary.\n",
    "\n",
    "$\\alpha = 1$ ($\\alpha$ is a smoothing parameter).\n",
    "\n",
    "To calculate all these probabilities, we'll first need to perform a bit of data cleaning to bring the data in a format that will allow us to extract easily all the information we need.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e519c887",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "We first need to strip out the punctuation from the SMS messages. Punctuation would effectively make different words, e.g. money, money!, money?, money!!!, would all be treated as different words.\n",
    "### Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61914b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yep, by the pretty sculpture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yes, princess. Are you going to make me moan?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Welp apparently he retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Havent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I forgot 2 ask ü all smth.. There's a card on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok i thk i got it. Then u wan me 2 come now or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>I want kfc its Tuesday. Only buy 2 meals ONLY ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>No dear i was sleeping :-P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok pa. Nothing problem:-)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ill be there on  &amp;lt;#&amp;gt;  ok.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham                       Yep, by the pretty sculpture\n",
       "1   ham      Yes, princess. Are you going to make me moan?\n",
       "2   ham                         Welp apparently he retired\n",
       "3   ham                                            Havent.\n",
       "4   ham  I forgot 2 ask ü all smth.. There's a card on ...\n",
       "5   ham  Ok i thk i got it. Then u wan me 2 come now or...\n",
       "6   ham  I want kfc its Tuesday. Only buy 2 meals ONLY ...\n",
       "7   ham                         No dear i was sleeping :-P\n",
       "8   ham                          Ok pa. Nothing problem:-)\n",
       "9   ham                    Ill be there on  &lt;#&gt;  ok."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Looking at the data before the punctuation removed.\n",
    "display(training_ds.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35ae3595",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using Series.str.replace() combined with regex pattern \\W, to remove all punctuaation.\n",
    "\n",
    "training_ds['SMS'] = training_ds['SMS'].str.replace('\\W', ' ')\n",
    "test_ds['SMS'] = test_ds['SMS'].str.replace('\\W', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab218396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4448</th>\n",
       "      <td>ham</td>\n",
       "      <td>I donno its in your genes or something</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4449</th>\n",
       "      <td>spam</td>\n",
       "      <td>YOUR CHANCE TO BE ON A REALITY FANTASY SHOW ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4450</th>\n",
       "      <td>ham</td>\n",
       "      <td>Prakesh is there know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4451</th>\n",
       "      <td>ham</td>\n",
       "      <td>The beauty of life is in next second   which h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4452</th>\n",
       "      <td>ham</td>\n",
       "      <td>How about clothes  jewelry  and trips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4453</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry  I ll call later in meeting any thing re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4454</th>\n",
       "      <td>ham</td>\n",
       "      <td>Babe  I fucking love you too    You know  Fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4455</th>\n",
       "      <td>spam</td>\n",
       "      <td>U ve been selected to stay in 1 of 250 top Bri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4456</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hello my boytoy     Geeee I miss you already a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4457</th>\n",
       "      <td>ham</td>\n",
       "      <td>Wherre s my boytoy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                                SMS\n",
       "4448   ham             I donno its in your genes or something\n",
       "4449  spam  YOUR CHANCE TO BE ON A REALITY FANTASY SHOW ca...\n",
       "4450   ham                             Prakesh is there know \n",
       "4451   ham  The beauty of life is in next second   which h...\n",
       "4452   ham             How about clothes  jewelry  and trips \n",
       "4453   ham  Sorry  I ll call later in meeting any thing re...\n",
       "4454   ham  Babe  I fucking love you too    You know  Fuck...\n",
       "4455  spam  U ve been selected to stay in 1 of 250 top Bri...\n",
       "4456   ham  Hello my boytoy     Geeee I miss you already a...\n",
       "4457   ham                           Wherre s my boytoy      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking that the data has had the punctuation removed.\n",
    "display(training_ds.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e76e3c",
   "metadata": {},
   "source": [
    "### Capitalisation / Case\n",
    "\n",
    "In the same vein, we want to make sure that all the words are in a consistent case. Capitalisation can effectively make different words, e.g. Money, money, mOney, MONEY, would all be treated as different words. We are going to convert all the words to a lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ed51647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Series.str.lower() to convert our SMS words to lower case.\n",
    "\n",
    "test_ds['SMS'] = test_ds['SMS'].str.lower()\n",
    "training_ds['SMS'] = training_ds['SMS'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b21688d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4448</th>\n",
       "      <td>ham</td>\n",
       "      <td>i donno its in your genes or something</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4449</th>\n",
       "      <td>spam</td>\n",
       "      <td>your chance to be on a reality fantasy show ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4450</th>\n",
       "      <td>ham</td>\n",
       "      <td>prakesh is there know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4451</th>\n",
       "      <td>ham</td>\n",
       "      <td>the beauty of life is in next second   which h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4452</th>\n",
       "      <td>ham</td>\n",
       "      <td>how about clothes  jewelry  and trips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4453</th>\n",
       "      <td>ham</td>\n",
       "      <td>sorry  i ll call later in meeting any thing re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4454</th>\n",
       "      <td>ham</td>\n",
       "      <td>babe  i fucking love you too    you know  fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4455</th>\n",
       "      <td>spam</td>\n",
       "      <td>u ve been selected to stay in 1 of 250 top bri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4456</th>\n",
       "      <td>ham</td>\n",
       "      <td>hello my boytoy     geeee i miss you already a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4457</th>\n",
       "      <td>ham</td>\n",
       "      <td>wherre s my boytoy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                                SMS\n",
       "4448   ham             i donno its in your genes or something\n",
       "4449  spam  your chance to be on a reality fantasy show ca...\n",
       "4450   ham                             prakesh is there know \n",
       "4451   ham  the beauty of life is in next second   which h...\n",
       "4452   ham             how about clothes  jewelry  and trips \n",
       "4453   ham  sorry  i ll call later in meeting any thing re...\n",
       "4454   ham  babe  i fucking love you too    you know  fuck...\n",
       "4455  spam  u ve been selected to stay in 1 of 250 top bri...\n",
       "4456   ham  hello my boytoy     geeee i miss you already a...\n",
       "4457   ham                           wherre s my boytoy      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking that the data has been converted to lower case.\n",
    "display(training_ds.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c71f0c8",
   "metadata": {},
   "source": [
    "## Creating a Vocabulary\n",
    "\n",
    "Now that we have standardised the SMS messages, we want to create a Vocabulary for our messages by splitting the SMS strings into individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1280162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>Word List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>yep  by the pretty sculpture</td>\n",
       "      <td>[yep, by, the, pretty, sculpture]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>yes  princess  are you going to make me moan</td>\n",
       "      <td>[yes, princess, are, you, going, to, make, me,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>welp apparently he retired</td>\n",
       "      <td>[welp, apparently, he, retired]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>havent</td>\n",
       "      <td>[havent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>i forgot 2 ask ü all smth   there s a card on ...</td>\n",
       "      <td>[i, forgot, 2, ask, ü, all, smth, there, s, a,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS  \\\n",
       "0   ham                       yep  by the pretty sculpture   \n",
       "1   ham      yes  princess  are you going to make me moan    \n",
       "2   ham                         welp apparently he retired   \n",
       "3   ham                                            havent    \n",
       "4   ham  i forgot 2 ask ü all smth   there s a card on ...   \n",
       "\n",
       "                                           Word List  \n",
       "0                  [yep, by, the, pretty, sculpture]  \n",
       "1  [yes, princess, are, you, going, to, make, me,...  \n",
       "2                    [welp, apparently, he, retired]  \n",
       "3                                           [havent]  \n",
       "4  [i, forgot, 2, ask, ü, all, smth, there, s, a,...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the SMS messages into a list of words and assinging to a new column called 'Word List'\n",
    "training_ds['Word List'] = training_ds['SMS'].str.split()\n",
    "\n",
    "training_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1de170dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a loop and nested loop to go through each SMS and add the individual words to a vocabulary list.\n",
    "\n",
    "vocab = []\n",
    "\n",
    "for sms in training_ds['Word List']:\n",
    "    for word in sms:\n",
    "        vocab.append(word)\n",
    "\n",
    "# Converting this list to a set to remove duplicate words, and returning to a list.\n",
    "\n",
    "vocab = list(set(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0a55d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['idk', 'listened2the', 'doke', 'lengths', 'weird']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sampling five random entries from the vacabulary\n",
    "import random\n",
    "\n",
    "random.sample(vocab,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662d3fb9",
   "metadata": {},
   "source": [
    "## Creating a Word Count Dictionary for All SMS Messages\n",
    "\n",
    "Now we're going to use the vocabulary to calculate how often each word occurs in each SMS message. Eventually, we're going to create a new DataFrame. However, we'll first build a dictionary that we'll then convert to the DataFrame we need.\n",
    "\n",
    "To create the dictionary we need for our training set, we can use the code below, where:\n",
    "\n",
    "- We start by initializing a dictionary named word_counts_per_sms, where each key is a unique word (a string) from the vocabulary, and each value is a list of the length of training set, where each element in the list is a 0.\n",
    "\n",
    "    - The code [0] * len(training_ds['Word List']) outputs a list of the length of training_ds['Word List'], where each element in the list will be a 0.\n",
    "- We loop over training_ds['Word List'] using at the same time the enumerate() function to get both the index and the SMS message (index and sms).\n",
    "\n",
    "    - Using a nested loop, we loop over SMS (where SMS is a list of strings, where each string represents a word in a message).\n",
    "        - We incremenent word_counts_per_sms[word][index] by 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd02b9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by initialing a dictionary with keys being the vocabulary words.\n",
    "# The entries under different keys (vocab words) are lists of numbers, correlating with how often that word occurs \n",
    "# in respective SMS\n",
    "\n",
    "word_counts_per_sms = {unique_word: [0] * len(training_ds['Word List']) for unique_word in vocab}\n",
    "\n",
    "# We now iterate through all the SMS messages to tally up the occurence of words in each message \n",
    "for index, sms in enumerate(training_ds['Word List']):\n",
    "    \n",
    "    # Iterating through the words in each SMS and increasing the value in the respective list object within the word key.\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0862cb",
   "metadata": {},
   "source": [
    "Looking at the dictionary entry for the word __the__, we see the corresponding tallies for each different SMS message in the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dff63ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts_per_sms['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6a16674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the dictionary to a more usable DataFrame\n",
    "\n",
    "word_counts_per_sms_df = pd.DataFrame(word_counts_per_sms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e828a2f3",
   "metadata": {},
   "source": [
    "Concatenating the DataFrame we just built above with the DataFrame containing the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b10a189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the original dataset with the individual word counts in each message\n",
    "\n",
    "training_ds_wc = pd.concat([training_ds,word_counts_per_sms_df],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7d2a55",
   "metadata": {},
   "source": [
    "We are going to look at a specific message to highlight what this new DataFrame contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30190281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i m going to try for 2 months ha ha only joking'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at a message with the index number 1,000 (arbitrarily choosen)\n",
    "training_ds_wc.iloc[1000,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7d51ee",
   "metadata": {},
   "source": [
    "The columns we concatenated to the original training DataFrame have tallies for how often our vocabulary words have occured in this message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c134cf6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word ha occurs: 2 times\n",
      "The word joking occurs: 1 times\n",
      "The word fish occurs: 0 times\n"
     ]
    }
   ],
   "source": [
    "# Looking at the occurence of 'ha' in the message above (index number 1,000)\n",
    "print('The word ha occurs: ' + str(training_ds_wc['ha'].iloc[1000]) + ' times')\n",
    "\n",
    "# Looking at the occurence of 'joking' in the message above (index number 1,000)\n",
    "print('The word joking occurs: ' + str(training_ds_wc['joking'].iloc[1000]) + ' times')\n",
    "\n",
    "# Looking at the occurence of 'fish' in the message above (index number 1,000)\n",
    "print('The word fish occurs: ' + str(training_ds_wc['fish'].iloc[1000]) + ' times')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf562fd",
   "metadata": {},
   "source": [
    "## Creating the Spam Filter\n",
    "\n",
    "Now that we're done with data cleaning and have a training set to work with, we can begin creating the spam filter. Recall that the Naive Bayes algorithm will need to know the probability values of the two equations below to be able to classify new messages:\n",
    "\n",
    "$$P(Spam|w_1,w_2,...,w_n)\\propto P(Spam)\\cdot\\prod_{i=1}^n P(w_i|Spam)$$\n",
    "\n",
    "$$P(Ham|w_1,w_2,...,w_n)\\propto P(Ham)\\cdot\\prod_{i=1}^n P(w_i|Ham)$$\n",
    "\n",
    "Also, to calculate P(wi|Spam) and P(wi|Ham) inside the formulas above, we need to use these equations:\n",
    "\n",
    "$$P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha\\cdot N_{Vocabulary}}$$\n",
    "\n",
    "$$P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha\\cdot N_{Vocabulary}}$$\n",
    "\n",
    "Some of the terms in the four equations above will have the same value for every new message. As a start, let's first calculate:\n",
    "- $P(Spam)$ and $P(Ham)$\n",
    "- $N_{Ham}, N_{Spam}, N_{Vocabulary}$\n",
    "\n",
    "We'll also use Laplace smoothing and set $\\alpha = 1$.\n",
    "\n",
    "## Calculating Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5088a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.86541\n",
       "spam    0.13459\n",
       "Name: Label, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of a non-spam SMS is 0.8654104979811574, and a spam SMS is 0.13458950201884254.\n"
     ]
    }
   ],
   "source": [
    "# Calculating P(Spam) and P(Ham) using the training dataset. We use the normalise function in value_counts to get the\n",
    "# proportions of each mail type, which is also the probabilty of getting either type of message within on training dataset.\n",
    "\n",
    "display(training_ds_wc['Label'].value_counts(normalize=True))\n",
    "\n",
    "# Using indexing to extract the probabilities for ham and spam\n",
    "\n",
    "p_ham = training_ds_wc['Label'].value_counts(normalize=True)[0]\n",
    "p_spam = training_ds_wc['Label'].value_counts(normalize=True)[1]\n",
    "\n",
    "print('The probability of a non-spam SMS is ' + str(p_ham) + ', and a spam SMS is ' + str(p_spam) + '.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f08e763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_total (The total number of words in the training texts) is: 72427\n",
      "n_spam (The total number of words in spam SMS within the training dataset) is: 15190\n",
      "N_ham (The total number of words in non-spam SMS within the training dataset) is: 57237\n",
      "N_vocab (The total number of words in the training dataset vocabulary) is: 7783\n"
     ]
    }
   ],
   "source": [
    "# Calculating N_spam, N_ham and N_vocab\n",
    "\n",
    "# First of all we are going to divide the training dataset into spam and ham categories\n",
    "training_ds_wc_spam = training_ds_wc[training_ds_wc['Label'] == 'spam']\n",
    "training_ds_wc_ham = training_ds_wc[training_ds_wc['Label'] == 'ham']\n",
    "\n",
    "\n",
    "# N_total is equal to the number of words in all the messages - not the number of messages\n",
    "# Setting a counter for the total number of words in all the messages as n_total\n",
    "n_total = 0\n",
    "\n",
    "# Looping through all unique words and adding their total mentions to the n_total counter\n",
    "for word in vocab:\n",
    "    n_total += sum(training_ds_wc[word])\n",
    "\n",
    "print('n_total (The total number of words in the training texts) is: ' + str(n_total))\n",
    "\n",
    "# N_Spam is equal to the number of words in all the spam messages - not the number of spam messages\n",
    "# Setting a counter for the total number of words in all the spam messages as n_spam\n",
    "\n",
    "n_spam = 0\n",
    "\n",
    "# Looping through all unique words and adding their total mentions in spam messages to the n_spam counter\n",
    "for word in vocab:\n",
    "    n_spam += sum(training_ds_wc_spam[word])\n",
    "    \n",
    "# Alternative way to calculate:\n",
    "#spam_words = training_ds_wc_spam['Word List'].apply(len)\n",
    "#n_spam = spam_words.sum()\n",
    "    \n",
    "print('n_spam (The total number of words in spam SMS within the training dataset) is: ' + str(n_spam))\n",
    "\n",
    "# N_Ham is equal to the number of words in all the non-spam messages - not the number of non-spam messages\n",
    "# Setting a counter for the total number of words in all the non-spam messages as n_ham\n",
    "\n",
    "n_ham = 0\n",
    "\n",
    "# Looping through all unique words and adding their total mentions in non-spam messages to the n_ham counter\n",
    "for word in vocab:\n",
    "    n_ham += sum(training_ds_wc_ham[word])\n",
    "    \n",
    "# Alternative way to calculate:\n",
    "#ham_words = training_ds_wc_ham['Word List'].apply(len)\n",
    "#n_ham = ham_words.sum()\n",
    "\n",
    "print('N_ham (The total number of words in non-spam SMS within the training dataset) is: ' + str(n_ham))\n",
    "\n",
    "# We want to calculate the number of unique words in the vocabulary, n_vocab\n",
    "n_vocab = len(vocab)\n",
    "\n",
    "print('N_vocab (The total number of words in the training dataset vocabulary) is: ' + str(n_vocab))\n",
    "\n",
    "# Lastly we want to set a laplace smoothing coeffecient \n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071de35a",
   "metadata": {},
   "source": [
    "## Calculating Conditional Probability Parameters\n",
    "\n",
    "Let's now calculate $P(w_i|Spam)$ (the probability of a given word occuring, given the message is spam) and $P(w_i|Ham)$ (the probability of a given word occuring, given the message is not spam) for all the words in the training vocabulary, using the equations below:\n",
    "\n",
    "$$P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha\\cdot N_{Vocabulary}}$$\n",
    "\n",
    "$$P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha\\cdot N_{Vocabulary}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e54fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize two dictionaries, where each key-value pair is a unique word (from our vocabulary) represented as a string.\n",
    "# We'll need one dictionary to store the parameters for P(wi|Spam), and the other for P(wi|Ham).\n",
    "\n",
    "p_word_given_spam = {word: [0] for word in vocab}\n",
    "p_word_given_ham = {word: [0] for word in vocab}\n",
    "\n",
    "# Looping through all unique words in the vocabulary and calculating the probility that word will occur given the\n",
    "# nature of the SMS (spam or non-spam), and assigning that probabilty to the dictionary\n",
    "for word in vocab:\n",
    "    p_word_given_spam[word] = (training_ds_wc_spam[word].sum() + alpha) / (n_spam + alpha * n_vocab)\n",
    "    p_word_given_ham[word] = (training_ds_wc_ham[word].sum() + alpha) / (n_ham + alpha * n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223ecd1d",
   "metadata": {},
   "source": [
    "## Testing Word Probabilites\n",
    "Looking at a few words and their probabilities of occuring in either spam on non-spam SMS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "951a6db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word \"offer\" occurs in\"0.011% of non-spam messages.\n",
      "The word \"offer\" occurs in 0.096% of spam messages.\n",
      "\n",
      "The word \"urgent\" occurs in\"0.009% of non-spam messages.\n",
      "The word \"urgent\" occurs in 0.209% of spam messages.\n",
      "\n",
      "The word \"important\" occurs in 0.017% of non-spam messages.\n",
      "The word \"important\" occurs in 0.048% of spam messages.\n",
      "\n",
      "The word \"thanks\" occurs in 0.082% of non-spam messages.\n",
      "The word \"thanks\" occurs in 0.061% of spam messages.\n",
      "\n",
      "The word \"love\" occurs in 0.252% of non-spam messages.\n",
      "The word \"love\" occurs in 0.044% of spam messages.\n",
      "\n",
      "The word \"careful\" occurs in 0.252% of non-spam messages.\n",
      "The word \"careful\" occurs in 0.044% of spam messages.\n"
     ]
    }
   ],
   "source": [
    "print('The word \\\"offer\\\" occurs in\"' + str(round(p_word_given_ham['offer'] * 100,3)) + '% of non-spam messages.')\n",
    "print('The word \\\"offer\\\" occurs in ' + str(round(p_word_given_spam['offer'] * 100,3)) + '% of spam messages.')\n",
    "print('')\n",
    "print('The word \\\"urgent\\\" occurs in\"' + str(round(p_word_given_ham['urgent'] * 100,3)) + '% of non-spam messages.')\n",
    "print('The word \\\"urgent\\\" occurs in ' + str(round(p_word_given_spam['urgent'] * 100,3)) + '% of spam messages.')\n",
    "print('')\n",
    "print('The word \\\"important\\\" occurs in ' + str(round(p_word_given_ham['important'] * 100,3)) + '% of non-spam messages.')\n",
    "print('The word \\\"important\\\" occurs in ' + str(round(p_word_given_spam['important'] * 100,3)) + '% of spam messages.')\n",
    "print('')\n",
    "print('The word \\\"thanks\\\" occurs in ' + str(round(p_word_given_ham['thanks'] * 100,3)) + '% of non-spam messages.')\n",
    "print('The word \\\"thanks\\\" occurs in ' + str(round(p_word_given_spam['thanks'] * 100,3)) + '% of spam messages.')\n",
    "print('')\n",
    "print('The word \\\"love\\\" occurs in ' + str(round(p_word_given_ham['love'] * 100,3)) + '% of non-spam messages.')\n",
    "print('The word \\\"love\\\" occurs in ' + str(round(p_word_given_spam['love'] * 100,3)) + '% of spam messages.')\n",
    "print('')\n",
    "print('The word \\\"careful\\\" occurs in ' + str(round(p_word_given_ham['love'] * 100,3)) + '% of non-spam messages.')\n",
    "print('The word \\\"careful\\\" occurs in ' + str(round(p_word_given_spam['love'] * 100,3)) + '% of spam messages.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a840314f",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "We can see that imperative words such as _important_, _urgent_ and _offer_ are much more likely to occur in spam messages. This is often a tactic of spam messages to try an encourage engagement by making it seem that there is a sense of urgency in reacting to their messages. Often this can be in conjuction with scamming and monetary persuit.\n",
    "\n",
    "In contrast, more personable words such as _love_, _thanks_ and _careful_ are more likely to occur in non-spam messages. This is likely because these words represent much more genuine interactions between people who are familiar with one another already."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9cbfe5",
   "metadata": {},
   "source": [
    "## Creating a Function To Calculate If a SMS is Spam or Not\n",
    "\n",
    "We now have all the parameters required to determine whether a new SMS message is more likely to be genuine or spam, using the equations below:\n",
    "\n",
    "$$P(Spam|w_1,w_2,...,w_n)\\propto P(Spam)\\cdot\\prod_{i=1}^n P(w_i|Spam)$$\n",
    "\n",
    "$$P(Ham|w_1,w_2,...,w_n)\\propto P(Ham)\\cdot\\prod_{i=1}^n P(w_i|Ham)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ed1c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a function to determine whether a message is spam or not.\n",
    "# This function takes in a text message (SMS) and uses the product probability equations above\n",
    "\n",
    "def determine_spam(SMS):\n",
    "    # First we do some data cleaning to remove any punctuation using regex\n",
    "    SMS_np = re.sub(r'[^\\w\\s]', '', SMS)\n",
    "    # We then make all the words lowercase to match the casing of our dictionaries\n",
    "    SMS_np_lc = SMS_np.lower()\n",
    "    # Then we split the SMS string into a list, which allows us to iterate over each word in the SMS\n",
    "    SMS_word_list = SMS_np_lc.split()\n",
    "    \n",
    "    # We set the probabilities for spam and non-spam messages as one, as we will multiply these by the probabilities \n",
    "    # of each word.\n",
    "    non_spam_probability = p_ham\n",
    "    spam_probability = p_spam\n",
    "    \n",
    "    # Now we cycle through each word in the input SMS and then multiply the probabilities of spam and non-spam,\n",
    "    # for the scenarios that the message is spam or non-spam,\n",
    "    for word in SMS_word_list:\n",
    "\n",
    "        if word in p_word_given_spam:\n",
    "            non_spam_probability *= p_word_given_ham[word]\n",
    "        if word in p_word_given_ham:\n",
    "            spam_probability *= p_word_given_spam[word]\n",
    "    \n",
    "    # Once we have iterated over all the words in the message, we compare the probabilities that the message is\n",
    "    # spam or non-spam, to give the user a message indicating what the message is more likely to be.\n",
    "    \n",
    "    if non_spam_probability < spam_probability:\n",
    "        print('P(Spam|text) is ' + str(spam_probability*100) + '%') \n",
    "        print('P(Ham|text) is ' + str(non_spam_probability*100) + '%') \n",
    "        print('\\\"' + str(SMS) + '\\\"' + ' is a spam message')\n",
    "    else:\n",
    "        print('P(Spam|text) is ' + str(spam_probability*100) + '%') \n",
    "        print('P(Ham|text) is ' + str(non_spam_probability*100) + '%') \n",
    "        print('\\\"' + str(SMS) + '\\\"' + ' is not a spam message')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d813ca",
   "metadata": {},
   "source": [
    "## Testing the Spam Classification Function\n",
    "\n",
    "With the function written we can generate some test text messages. One which is very typically spam and the other which is a genuine message between two people who know one another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04c413e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|text) is 1.2824234293423733e-27%\n",
      "P(Ham|text) is 3.503014781000854e-23%\n",
      "\"hey, should we grab a beer after work?\" is not a spam message\n"
     ]
    }
   ],
   "source": [
    "# Non-spam message\n",
    "SMS_message = 'hey, should we grab a beer after work?'\n",
    "\n",
    "# We can now run the function on the sample SMS messages\n",
    "determine_spam(SMS_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d56e1399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|text) is 1.3481290211300842e-23%\n",
      "P(Ham|text) is 1.9368049028589874e-25%\n",
      "\"WINNER!! This is the secret code to unlock the money: C3421\" is a spam message\n"
     ]
    }
   ],
   "source": [
    "# Spam message\n",
    "SMS_message = 'WINNER!! This is the secret code to unlock the money: C3421'\n",
    "\n",
    "# We can now run the function on the sample SMS messages\n",
    "determine_spam(SMS_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d366a",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "We can see that the function is able to distinguish between these two messages using the conditional probabilities determined for the message, using the pre-determined parameters of the training dataset.\n",
    "\n",
    "## Application to the Test Dataset\n",
    "\n",
    "We can now modify this function to determine whether the messages in the training dataset are spam on not. Instead of returning a printed message, we will return a label <mark>ham</mark> or <mark>spam</mark>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6549a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_spam(SMS):\n",
    "    # First we do some data cleaning to remove any punctuation using regex\n",
    "    SMS_np = re.sub(r'[^\\w\\s]', '', SMS)\n",
    "    # We then make all the words lowercase to match the casing of our dictionaries\n",
    "    SMS_np_lc = SMS_np.lower()\n",
    "    # Then we split the SMS string into a list, which allows us to iterate over each word in the SMS\n",
    "    SMS_word_list = SMS_np_lc.split()\n",
    "    \n",
    "    # We set the probabilities for spam and non-spam messages as one, as we will multiply these by the probabilities \n",
    "    # of each word.\n",
    "    non_spam_probability = 1\n",
    "    spam_probability = 1\n",
    "    \n",
    "    # Now we cycle through each word in the input SMS and then multiply the probabilities of spam and non-spam,\n",
    "    # for the scenarios that the message is spam or non-spam,\n",
    "    for word in SMS_word_list:\n",
    "        if word in p_word_given_spam:\n",
    "            non_spam_probability *= p_word_given_ham[word]\n",
    "        if word in p_word_given_ham:\n",
    "            spam_probability *= p_word_given_spam[word]\n",
    "    \n",
    "    # Once we have iterated over all the words in the message, we compare the probabilities that the message is\n",
    "    # spam or non-spam, to return a label or 'ham' or 'spam'.\n",
    "    \n",
    "    if non_spam_probability < spam_probability:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'ham'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97ec3ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds['New Classification'] = test_ds['SMS'].apply(classify_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73ab6941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     952\n",
       "spam    162\n",
       "Name: New Classification, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds['New Classification'].value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4318480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     967\n",
       "spam    147\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds['Label'].value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3aabda66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our algorithm correctly identified the nature of 1089 messages.\n",
      "Our algorithm incorrectly identified the nature of 25 messages.\n",
      "This is an accuracy of 0.978%.\n"
     ]
    }
   ],
   "source": [
    "# Looping through our test dataset to count how many are correct and how many are incorrect\n",
    "\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "total = test_ds.shape[0]\n",
    "\n",
    "\n",
    "for row in test_ds.iterrows():\n",
    "    row=row[1]\n",
    "    if row['Label'] == row['New Classification']:\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1\n",
    "        \n",
    "print('Our algorithm correctly identified the nature of ' + str(correct) + ' messages.')\n",
    "print('Our algorithm incorrectly identified the nature of ' + str(incorrect) + ' messages.')\n",
    "print('This is an accuracy of ' + str(round((correct/total),3)) + '%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e232221f",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We can see that our algorithm has correctly labelled 98% of the test dataset as either spam or non-spam messages.\n",
    "\n",
    "## Investigating the Incorrect Labelling\n",
    "\n",
    "To finish we are going to look at the mislabelled text messages to figure out why they were incorrectly identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "689eec4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>New Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ham</td>\n",
       "      <td>i liked the new mobile</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>spam</td>\n",
       "      <td>not heard from u4 a while  call me now am here...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>ham</td>\n",
       "      <td>unlimited texts  limited minutes</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>ham</td>\n",
       "      <td>26th of july</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>ham</td>\n",
       "      <td>surely result will offer</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>ham</td>\n",
       "      <td>which channel</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>ham</td>\n",
       "      <td>nokia phone is lovly</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>ham</td>\n",
       "      <td>no calls  messages  missed calls</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>ham</td>\n",
       "      <td>this phone has the weirdest auto correct</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>ham</td>\n",
       "      <td>we have sent jd for customer service cum accou...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>ham</td>\n",
       "      <td>just taste fish curry   p</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>ham</td>\n",
       "      <td>no  yes please  been swimming</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>ham</td>\n",
       "      <td>hasn t that been the pattern recently crap wee...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>ham</td>\n",
       "      <td>madam regret disturbance might receive a refer...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>spam</td>\n",
       "      <td>oh my god  i ve found your number again  i m s...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>spam</td>\n",
       "      <td>hi babe its chloe  how r u  i was smashed on s...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>ham</td>\n",
       "      <td>staff science nus edu sg  phyhcmk teaching pc1323</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>ham</td>\n",
       "      <td>garbage bags  eggs  jam  bread  hannaford whea...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>ham</td>\n",
       "      <td>networking technical support associate</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>ham</td>\n",
       "      <td>gibbs unsold mike hussey</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>ham</td>\n",
       "      <td>except theres a chick with huge boobs</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>spam</td>\n",
       "      <td>0a networks allow companies to bill for sms  s...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>spam</td>\n",
       "      <td>hello  we need some posh birds and chaps to us...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>ham</td>\n",
       "      <td>raviyog peripherals bhayandar east</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>ham</td>\n",
       "      <td>i  career tel  have added u as a contact on in...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                                SMS  \\\n",
       "9      ham                             i liked the new mobile   \n",
       "114   spam  not heard from u4 a while  call me now am here...   \n",
       "152    ham                  unlimited texts  limited minutes    \n",
       "159    ham                                       26th of july   \n",
       "182    ham                         surely result will offer     \n",
       "247    ham                          which channel               \n",
       "284    ham                             nokia phone is lovly     \n",
       "302    ham                   no calls  messages  missed calls   \n",
       "304    ham          this phone has the weirdest auto correct    \n",
       "319    ham  we have sent jd for customer service cum accou...   \n",
       "331    ham                          just taste fish curry   p   \n",
       "351    ham                     no  yes please  been swimming    \n",
       "398    ham  hasn t that been the pattern recently crap wee...   \n",
       "492    ham  madam regret disturbance might receive a refer...   \n",
       "504   spam  oh my god  i ve found your number again  i m s...   \n",
       "546   spam  hi babe its chloe  how r u  i was smashed on s...   \n",
       "605    ham  staff science nus edu sg  phyhcmk teaching pc1323   \n",
       "667    ham  garbage bags  eggs  jam  bread  hannaford whea...   \n",
       "689    ham            networking technical support associate    \n",
       "706    ham                           gibbs unsold mike hussey   \n",
       "723    ham             except theres a chick with huge boobs    \n",
       "741   spam  0a networks allow companies to bill for sms  s...   \n",
       "953   spam  hello  we need some posh birds and chaps to us...   \n",
       "983    ham                 raviyog peripherals bhayandar east   \n",
       "1073   ham  i  career tel  have added u as a contact on in...   \n",
       "\n",
       "     New Classification  \n",
       "9                  spam  \n",
       "114                 ham  \n",
       "152                spam  \n",
       "159                spam  \n",
       "182                spam  \n",
       "247                spam  \n",
       "284                spam  \n",
       "302                spam  \n",
       "304                spam  \n",
       "319                spam  \n",
       "331                spam  \n",
       "351                spam  \n",
       "398                spam  \n",
       "492                spam  \n",
       "504                 ham  \n",
       "546                 ham  \n",
       "605                spam  \n",
       "667                spam  \n",
       "689                spam  \n",
       "706                spam  \n",
       "723                spam  \n",
       "741                 ham  \n",
       "953                 ham  \n",
       "983                spam  \n",
       "1073               spam  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds[test_ds['Label'] != test_ds['New Classification']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af464741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
